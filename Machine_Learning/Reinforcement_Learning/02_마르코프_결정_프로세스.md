# CH02. 마르코프 결정 프로세스(MDP)

## 2.1. 마르코프 프로세스

<p align='center'>
<img width="350" height="250" alt="Image" src="https://github.com/user-attachments/assets/70b8175f-53ae-4d4d-a3a0-668244dbf235" /></p>

\* $S_0$: 모든 여정의 시작, $S_4$: 종료상태

- `상태 전이`: 현재 상태에서 다음 상태로 넘어간다는 것.
- $S_4$에 도달하는 순간 마르코프 프로세스는 끝.
- 미리 정의된 어떤 확률 분포를 따라서 상태와 상태 사이를 이동해 다니는 여정
    - 하나의 상태에서 뻗어 나가는 화살표의 합은 항상 100%
- 정의하기 위해 필요한 요소
- $MP≡(S, P)$
    - S: 상태의 집합  $S={S_0, S_1, S_2, S_3, S_4}$
    - P: 전이 확률 행렬  ($P_{ss'}$: s에서 s'에 도착할 확률)

$$P_{ss'}=P[S_{t+1}=s'|S_t=s]$$

- why 전이 확률 행렬?
    - $P_{ss'}$값을 각 상태 s와 s'에 대해 행렬의 형태로 표현 가능

    - ex) 앞 프로세스 그림과 호환
    
        |     |$S_0$|$S_1$|$S_2$|$S_3$|$S_4$|
        |-----|-----|-----|-----|-----|-----|
        |$S_0$|     |0.4  |0.6  |||
        |$S_1$|0.1  |0.9  ||||
        |$S_2$||||0.7|0.3|
        |$S_3$|||||1.0|
        |$S_4$|||||1.0|

### 마르코프 성질

- why 과정 이름이 "마르코프" 프로세스?
    - 마르코프 프로세스의 모든 상태가 마르코프 성질을 따르기 때문
    - $P[S_{t+1}|S_t]=P[S_{t+1}|S_1, S_2, ..., S_t]$
        
        "미래는 오로지 현재에 의해 결정된다."
        
        → $S_{t+1}$이 될 확률을 계산하려면 현재의 상태 $S_t$가 무엇인지만 주어지면 충분
        
        → 어떤 상태를 거쳐 왔든 현재 상태에서 다음 상태만 고려

- `마르코프한 상태`

    ex) 체스 게임
        
        → 당장 맞닥뜨린 상황을 잘 읽고 미래를 내다보며 최선의 수를 둬야 하는 것.
        → 어느 시점 t에 사진을 찍어서 사진만 보여주고 체스를 두라고 해도 손쉽게 어떤 수를 둘지 정할 수 있음.
        (+바둑도 마찬가지)

- `마르코프하지 않은 상태`
    
    ex) 운전

        →특정 시점 사진으로 운전을 해야할 때, 브레이크를 밟아야 할 지/엑셀을 밟아야 할 지 결정하기 힘듦.
        차가 앞으로 가는지 뒤로 가는지도 알 수 없음.
    
    |
    
    (10초 동안의 사진 10장 제공 or 사진 + 속도 벡터 + 가속도 벡터 등 제공)
    
    ↓

    마르코프 상태에 가까워짐!

<br>

\* 마르코프 프로세스 + 보상 ⇒ 마르코프 리워드 프로세스

<br>

## 2.2. 마르코프 리워드 프로세스

<p align='center'>
<img width="350" height="250" alt="Image" src="https://github.com/user-attachments/assets/e88752de-2e64-4dc9-9167-fb964ae79f57" />
</p>

- MRP를 정의하기 위해서는 R(보상함수)과 𝛾(감마) 2가지 요소 추가 필요

$$MRP≡(S, P, R, 𝛾)$$

- R: 보상함수 (어떤 상태 S에 도착했을 때 받는 보상)
$$R=E[R_t|S_t=S]$$

- 𝛾: 감쇠인자 (미래 보상에 곱해지며 값이 ↓)
    
    _0~1 사이 숫자_
    
    → 미래 얻을 보상에 비해 당장 얻는 보상을 얼마나 더 중요하게 여길 것인지를 나타내는 파라미터

    → 현재부터 미래에 얻게 될 보상의 합을 가리키는 `리턴` 이해 필요

### 감쇠된 보상의 합, 리턴

- 에피소드 - $S_0, R_0, S_1, R_1, ..., S_t, R_t$와 같은 하나의 여정
- 리턴: t 시점부터 미래에 받을 감쇠된 보상의 합

$$G_t = R_{t+1} + 𝛾R_{t+2} + 𝛾^2R_{t+3} + ...$$
$$(t:현재 타임스텝)$$

    → 𝛾이 여러번 곱해질수록 값이 0에 가까워짐.
    ⇒ 𝛾의 크기를 통해 미래에 얻게 될 보상에 비해 현재 얻는 보상에 가중치를 줄 수 있음.

- 강화학습은 보상이 아니라 리턴을 최대화하도록 학습하는 것
    - 에이전트의 목적은 지금부터 미래에 받을 보상의 합인 $G_t$를 최대화하는 것.

### 𝛾는 왜 필요할까?

- 𝛾은 미래를 평가 절하해주는 항

    → if 𝛾=0 ⇒ 보상 0 : 매우 근시안적인 Agent, 탐욕적.

    → if 𝛾=1 ⇒ 현재 보상 ←(대등)→ 미래 보상 : 매우 장기적인 시야의 Agent

- 수학적 편리성
    - 𝛾를 1보다 작게 해줌 → 리턴 $G_t$가 무한 값을 갖는 것 방지.
        
        (에피소드에서 얻는 각각의 보상의 최댓값이 정해져 있다면, $G_t$는 유한)

- 사람의 선호 반영
    - 5년 후 100만원보다는 당장 100만원 (눈앞의 보상 선호)

- 미래에 대한 불확실성 반영
    - 현재와 미래 사이에는 다양한 확률적 요소들이 있고, 이로 인해 당장 느끼는 가치에 비해 미래에느끼는 가치가 달라질 수 있음.

### MRP에서 각 상태의 밸류 평가하기

- 상태 $S_2$의 밸류 or 가치를 숫자 하나로 평가
    
        ex) 구글에서 앞으로 받을 연봉이 High (O)
            구글에 입사하기까지 많은 돈을 받아 (X)
            → 어떤 상태를 평가할 때에는 당연히 그 시점으로부터 미래에 일어날 보상을 기준으로 평가해야 함.

- 리턴 : 그 시점부터 이후에 받을 보상들을 (감쇠하여) 더한 값
    
    ⇒ $S_2$의 가치를 평가하고 싶다면 $S_2$부터 시작하여 리턴을 측정
    
    |
    
    (문제: 리턴값이 매번 바껴)

    ↓

    "리턴의 기대값을 사용"

### 에피소드의 샘플링

- 매번 에피소드가 어떻게 샘플링 되느냐에 따라 리턴이 달라짐.
- 샘플링 : 어떤 확률 분포가 있을 때 해당 분포에서 샘플을 뽑아보는 것
- Monte-Carlo 접근법 : 샘플링을 통해서 어떤값을 유추하는 방법론
    
    ex) 에피소드 샘플

    EP1 `누움` → `일어남` → `누움` → `눈 감음` → `잠이 옴` → `잠듦`

\* P가 주어져 있기 때문에 이런 샘플들을 원한다면 무한히 뽑아낼 수 있음.

### 상태 가치 함수

- 상태 가치 함수 : 상태를 Input으로 넣으면 그 상태의 밸류를 Output으로 출력하는 함수
    - $V(S)=E[G_t|S_t=S]$ : 상태 S로부터 시작하여 얻는 리턴의 기댓값
        - $S_t=S$: 시점 t에서 상태 S부터 시작하여 에피소드가 끝날 때까지의 리턴을 계산하라는 뜻 

    ex) EP1 `누움`($-1$) → `일어남`($+1\*0.9$) → `누움`($-1\*0.9^2$) → `눈 감음`($-1\*0.9^3$) → `잠이 옴`($+0\*0.9^4$) → `잠듦`($+10\*0.9^3$) = 4.3


- $S_0$에서 출발하여 발생할 수 있는 에피소드 무한히 多

    → 샘플로 얻는 리턴의 평균을 통해 밸류를 근사하게 Calculate

<br>

## 2.3. 마르코프 결정 프로세스

_순차적 의사결정에서 의사결정에 관한 부분이 모델에 포함_

_\* Agent 등장_

- MRP + Agent 등장 (Agent: 각 상황마다 액션 취함)
$$MDP≡(S, A, P, R, 𝛾)$$

- S: MP, MRP에서의 S와 같음
- A: 액션의 집합
    - Agent가 취할 수 있는 Action을 모아놓은 것
    - Agent는 Step마다 Action의 집합 중 하나 선택하여 Action → 다음에 도착하게 될 상태 달라짐.
- P: "현재 상태가 S이며 Agent가 액션 a를 선택했을 때 다음 상태가 S'이 될 확률"
    - ex) 징검다리, 같은 상태, 같은 액션, 환경(바람O, 바람X)
    - $P^a_{ss'}=P[S_{t+1}=s'|S_t=s,A_t=a]$
        - $S_t=s,A_t=a$: "현재 상태 s에서 액션 a를 했을 때"
- R: 액션의 영향 받음
    - $R^a_s=E[R_{t+1}|S_t=s,A_t=a]$
- 𝛾: MRP에서의 𝛾와 정확히 같음

### 아이 재우기 MDP

<p align='center'>
<img width="350" height="250" alt="Image" src="https://github.com/user-attachments/assets/810508b7-76d6-4409-9443-290b9eea9c86" />
</p>

- $P^a_{S_2,S_0}=0.3, P^{a_1}_{S_2,S_1}=0.7$

<p align='center'>
<img width="350" height="250" alt="Image" src="https://github.com/user-attachments/assets/3dbc01e2-58d6-422f-877f-f59b1b029a45" />
</p>

<p align='center'>
<img width="350" height="250" alt="Image" src="https://github.com/user-attachments/assets/e64b0dff-25f9-4206-816e-ca687552aeb6" />
</p>

- 정책
    - 복잡한 MDP에서 결국 우리가 찾고자 하는 것은 각 상태 S에 따라 어떤 액션 a를 선택해야 보상의 합을 최대화 할 수 있는가 이다.

### 정책 함수와 2가지 가치 함수

-  **정책 함수**: 각 상태에서 어떤 액션을 선택할지 정해주는 함수

    - ex) 어머니의 정책 → 어머니의 입장에서 아이의 상태에 따라 $a_0$를 선택할지 $a_1$을 선택할지 결정

        어머니의 목적 → 보상의 합을 최대화하는 정책을 찾는 것.   

    - $π(a|s)=P[A_t=a|S_t=s]$: 상태 s에서 액션 a를 선택할 확률

    \* 각 상태에서 할 수 있는 모든 액션의 확률값을 더하면 1이 되어야 함.

- 정책함수는 에이전트 안에 존재하는 점
- 환경은 변하지 않지만 Agent는 자신의 정책을 언제든 수정할 수 있음.
    
    → 더 큰 보상을 얻기 위해 계속해서 정책을 교정해 나가는 것이 곧 `강화학습`


    
-----
- **상태 가치 함수**
    - 가치 함수는 정책 함수에 의존적

    - 가치 함수를 정의하기 위해서는 먼저 정책 함수 π가 정의되어야 함.

    - $V_π(s)=E_π[𝛾_{t+1}+𝛾・𝛾_{t+2}+𝛾^2・𝛾_{t+3}+...|S_t=s]=E_π[G_t|
    S_t=s]$ → 정책함수를 π로 고정

        s부터 끝까지 π를 따라서 움직일 때 얻는 리턴의 기댓값
        
    \* 가치함수는 π에 의존적

----
- **액션 가치 함수 (상태-액션 가치 함수)**

    → 각 상태에서 선택할 수 있는 액션을 모두 평가해 본 다음에, 그중에 가장 가치있는 액션을 선택

    - $q_π(s,a)=E_π[G_t|S_t=s,A_t=a]$

        s에서 a를 선택하고, 그 이후에는 π를 따라서 움직일 때 얻는 리턴의 기댓값

<br>

## 2.4. Prediction과 Control (문제를 세팅하는 단계)

- $(S, A, P, R, 𝛾)$이 주어졌을 때 우리가 관심있는 Task?
    
    1. Prediction: π가 주어졌을 때 각 상태의 밸류($V_π(s)$)를 평가하는 문제
    2. Control: 최적 정책 $π^*$를 찾는 문제

- 예시 : 그리드 월드

    - \<Prediction>
        - 정책 π: for all s ∈ S
            
            $π(동|S)=0.25, π(서|S)=0.25, π(남|S)=0.25, π(북|S)=0.25$

        - Prediction 문제
            
            : $S_11$의 밸류 $V_π(S_{11})$의 값은 몇이나 될까요?

            → 해당 상태의 밸류를 예측하는 것이 목적

    - \<Control>
        - 최적의 정책 : 이 세상에 존재하는 모든 π 중에서 가장 기대 리턴이 큰 π
        
        - 최적 가치 함수($V^*$) : 최적 정책 $π^*$를 따를 때의 가치 함수