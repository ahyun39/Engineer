# CH03. 벨만 방정식 (재귀적 관계의 식)

_밸류를 계산하는데 있어 중요한 수식_

```
벨만 방정식
- 시점 t에서의 벨류 ↔︎ 시점 t+1에서의 밸류 사이 관계
- 가치 함수와 정책 함수 사이의 관계
```

- 재귀 함수
    - 벨만 방정식은 현재 시점(t)과 다음 시점(t+1) 사이의 재귀적 관계를 이용해 정의

## 3.1. 벨만 기대 방정식

- 0단계
    - $v_π(s_t)=𝔼_π[r_{t+1}+𝛾v_π(s_{t+1})]$
    - $q_π(s_t,a_t)=𝔼_π[r_{t+1}+𝛾q_π(s_{t+1},a_{t+1})]$

- 1단계
    - $v_π(s)=\sum_{a∈A}π(a|s)q_π(s,a)$
    - $q_π(s,a)=r_s^a+𝛾\sum_{s'∈S}P_{ss'}^av_π(s')$

- 2단계
    - $v_π(s)=\sum_{a∈A}π(a|s)\begin{pmatrix}r_s^a+\gamma\sum_{s'\in S}P_{ss'}^av_π(s')\end{pmatrix}$
    - $q_π(s,a)=r_s^a+\gamma\sum_{s'\in S}P_{ss'}^a \sum_{a'\in A}π(a'|s')q_π(s',a')$

### 0단계

$v_π(s_t)=𝔼_π[r_{t+1}+𝛾v_π(s_{t+1})]$

→ $v_π(s_t)=𝔼_π[G_t]$

ㅤㅤㅤㅤㅤ $=𝔼_π[r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+...]$

ㅤㅤㅤㅤㅤ $=𝔼_π[r_{t+1}+\gamma (r_{t+2}+\gamma r_{t+3}+...)]$

ㅤㅤㅤㅤㅤ $=𝔼_π[r_{t+1}+\gamma G_{t+1}]$

ㅤㅤㅤㅤㅤ $=𝔼_π[r_{t+1}+\gamma v_π(s_{t+1})]$

- Q1. $v_π(s_t)=r_{t+1}+\gamma v_π(s_{t+1})$ ? (`ans = X`)
    - why? 다음 상태인 $s_{t+1}$이 어디가 되느냐에 따라 $r_{t+1}$과 $v_π(s_{t+1})$의 값도 달라짐
    - img

- Q2. $v_π(s_t)=𝔼_π[r_{t+1}+\gamma r_{t+2}+\gamma^2v_π(s_{t+2})+...]$ ? (`ans = O`)
    - 현재로부터 한 스텝만 가서 미래 가치를 계산하든, 두 스텝을 가서 미래 가치를 계산하든 그 기댓값은 변하지 않는다.

### 1단계

1. $q_\pi$를 이용해 $v_\pi$ 계산하기

    - $v_\pi(s)=\sum_{a\in A}\pi(a|s)q_\pi(s,a)$
        - $v_\pi(s)$ : s의 밸류
        - $\pi(a|s)$ : s에서 a를 실행할 확률
        - $q_\pi(s,a)$ : s에서 a를 실행하는 것의 밸류
    
    - img
        - $v_\pi(s)=\pi(a_1|s)\*q_\pi(s,a_1)+\pi(a_2|s)\*q_\pi(s,a_2)=0.6\*1+0.4\*2=1.4=$
    
    \* 어떤 상태에서 선택할 수 있는 모든 액션의 밸류를 모두 알고 있다면, 이를 이용해 해당 상태의 밸류를 계산할 수 있음.

    (액션의 밸류에 액션을 선택할 확률을 곱해서 시그마를 이용해 모두 더해주는 방식)

2. 
